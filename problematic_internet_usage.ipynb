{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","from sklearn.base import clone\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import VotingRegressor\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures, LabelEncoder\n","from scipy.optimize import minimize\n","from scipy import stats\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm\n","from colorama import Fore, Style\n","from IPython.display import clear_output\n","from lightgbm import LGBMRegressor, LGBMClassifier\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Create output folders\n","output_folder = 'output'\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","# Create separate analysis output folders\n","analysis_output_folder = 'analysis_output'\n","os.makedirs(analysis_output_folder, exist_ok=True)\n","\n","physical_analysis_output_folder = 'analysis_output/physical'\n","os.makedirs(physical_analysis_output_folder, exist_ok=True)\n","\n","fitness_analysis_output_folder = 'analysis_output/fitness'\n","os.makedirs(fitness_analysis_output_folder, exist_ok=True)\n","\n","bia_analysis_output_folder = 'analysis_output/bia'\n","os.makedirs(bia_analysis_output_folder, exist_ok=True)\n","\n","child_info_analysis_output_folder = 'analysis_output/child_info'\n","os.makedirs(child_info_analysis_output_folder, exist_ok=True)\n","\n","actigraphy_analysis_output_folder = 'analysis_output/actigraphy'\n","os.makedirs(actigraphy_analysis_output_folder, exist_ok=True)\n","\n","# Load data functions\n","def process_file(filename, dirname):\n","    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n","    df.drop('step', axis=1, inplace=True)\n","    return df.describe().values.reshape(-1), filename.split('=')[1]\n","\n","def load_time_series(dirname) -> pd.DataFrame:\n","    ids = os.listdir(dirname)\n","    with ThreadPoolExecutor() as executor:\n","        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n","    stats, indexes = zip(*results)\n","    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n","    df['id'] = indexes\n","    return df\n","\n","# Set display all columns in dataframes property\n","pd.options.display.max_columns = None"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load data\n","train_data = pd.read_csv('input/train.csv')\n","test_data = pd.read_csv('input/test.csv')\n","sample_data = pd.read_csv('input/sample_submission.csv')\n","\n","train_ts_data = load_time_series(\"input/series_train.parquet\")\n","test_ts_data = load_time_series(\"input/series_test.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove id column from time series data\n","time_series_columns = train_ts_data.columns.tolist()\n","time_series_columns.remove(\"id\")\n","\n","# Merge data\n","train_data = pd.merge(train_data, train_ts_data, how=\"left\", on='id')\n","test_data = pd.merge(test_data, test_ts_data, how=\"left\", on='id')\n","train_data = train_data.drop('id', axis=1)\n","test_data = test_data.drop('id', axis=1)\n","\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature engineering\n","train_data = train_data.copy()\n","\n","# Combine all grip strength\n","train_data['FGC-FGC_GS'] = train_data['FGC-FGC_GSD_Zone'] + train_data['FGC-FGC_GSND_Zone']\n","\n","# Combine all sit and reach\n","train_data['FGC-FGC_SR'] = train_data['FGC-FGC_SRL_Zone'] + train_data['FGC-FGC_SRR_Zone']\n","\n","# Create a fitness score by adding the zone fitness data\n","train_data['fitness_score'] = train_data['FGC-FGC_GS'] + train_data['FGC-FGC_SR'] + train_data['FGC-FGC_CU_Zone'] + train_data['FGC-FGC_PU_Zone'] + train_data['FGC-FGC_TL_Zone']\n","\n","# Combine PAQ_A-PAQ_A_Total and PAQ_C-PAQ_C_Total into one column\n","train_data['PAQ_Total'] = train_data['PAQ_A-PAQ_A_Total'].combine_first(train_data['PAQ_C-PAQ_C_Total'])\n","\n","# Create interaction features\n","train_data['BIA_Fat_X_PCIAT_Total'] = train_data['BIA-BIA_Fat'] * train_data['PCIAT-PCIAT_Total']\n","train_data['Physical_Weight_X_BIA_BMI'] = train_data['Physical-Weight'] * train_data['BIA-BIA_BMI']\n","train_data['CGAS_Score_X_PCIAT_Total'] = train_data['CGAS-CGAS_Score'] * train_data['PCIAT-PCIAT_Total']\n","train_data['BIA_Fat_X_Physical_Weight'] = train_data['BIA-BIA_Fat'] * train_data['Physical-Weight']\n","train_data['BIA_BMI_X_PCIAT_Total'] = train_data['BIA-BIA_BMI'] * train_data['PCIAT-PCIAT_Total']\n","train_data['Physical_Weight_X_PCIAT_Total'] = train_data['Physical-Weight'] * train_data['PCIAT-PCIAT_Total']\n","\n","# Combine identical actigraphy stats based on correlation analysis\n","def combine_identical_features(df, columns_to_combine, new_column_name):\n","    # Verify if identical\n","    is_identical = df[columns_to_combine].nunique().eq(1).all()\n","    \n","    if is_identical:\n","        # If they are identical, we can just take the first column and rename it\n","        df[new_column_name] = df[columns_to_combine[0]]\n","        \n","        # Drop the original columns\n","        df = df.drop(columns=columns_to_combine)\n","        \n","        print(f\"Columns {columns_to_combine} have been combined into {new_column_name}\")\n","    else:\n","        # If not exactly identical, take the mean\n","        print(\"Warning: The columns are not identical. Taking the average instead.\")\n","        df[new_column_name] = df[columns_to_combine].mean(axis=1)\n","        \n","        # Drop the original columns\n","        df = df.drop(columns=columns_to_combine)\n","    \n","    return df\n","\n","# List of columns to combine\n","columns_to_combine = ['stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', \n","                      'stat_6', 'stat_7', 'stat_8', 'stat_9', 'stat_10', 'stat_11']\n","\n","# Apply to both train and test datasets\n","train_data = combine_identical_features(train_data, columns_to_combine, 'combined_actigraphy_stat')\n","\n","# Polynomial function\n","def create_polynomial_features(df, features, degrees):\n","    \"\"\"\n","    Create polynomial features for specified columns, handling NaN values.\n","    \n","    :param df: DataFrame\n","    :param features: List of feature names to create polynomials for\n","    :param degrees: Dictionary of feature names and their corresponding degrees\n","    :return: DataFrame with added polynomial features\n","    \"\"\"\n","    # Create a copy of the dataframe to avoid modifying the original\n","    df_poly = df.copy()\n","    \n","    for feature in features:\n","        max_degree = degrees.get(feature, 2)  # Default to degree 2 if not specified\n","        \n","        # Check for NaN values\n","        nan_count = df_poly[feature].isna().sum()\n","        if nan_count > 0:\n","            print(f\"Warning: {nan_count} NaN values found in {feature}. These will be imputed.\")\n","        \n","        # Impute NaN values with median\n","        imputer = SimpleImputer(strategy='median')\n","        df_poly[feature] = imputer.fit_transform(df_poly[[feature]])\n","        \n","        # Create polynomial features\n","        poly = PolynomialFeatures(degree=max_degree, include_bias=False)\n","        feature_poly = poly.fit_transform(df_poly[[feature]])\n","        \n","        # Create new column names\n","        feature_names = [f\"{feature}^{i}\" for i in range(1, max_degree + 1)]\n","        \n","        # Add new columns to the dataframe\n","        for i, name in enumerate(feature_names):\n","            df_poly[name] = feature_poly[:, i]\n","    \n","    return df_poly\n","\n","# Specify features and their degrees\n","features_to_transform = ['BIA-BIA_Fat', 'Physical-Weight']\n","degrees = {'BIA-BIA_Fat': 3, 'Physical-Weight': 2}\n","\n","# Apply to both train and test datasets\n","train_data = create_polynomial_features(train_data, features_to_transform, degrees)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Skew removal for some BIA columns\n","skewed_columns = [\n","    'BIA-BIA_BMC', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_Fat',\n","    'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', \n","    'BIA-BIA_TBW', 'CGAS-CGAS_Score', 'stat_23', 'stat_35', 'stat_38', 'stat_40', 'stat_47',\n","    'stat_54', 'stat_66', 'stat_78', 'stat_80', 'stat_88', 'stat_90'\n","]\n","lambda_params = {}\n","\n","# Define the box-cox function to remove skew\n","def box_cox_transform(df, column):\n","    # Create a copy of the dataframe\n","    df_copy = df.copy()\n","    \n","    # Drop NaN values for the specific column\n","    df_copy = df_copy.dropna(subset=[column])\n","    \n","    # Ensure all values are positive\n","    min_value = df_copy[column].min()\n","    if min_value <= 0:\n","        df_copy[column] = df_copy[column] - min_value + 1  # Add 1 to ensure all values are positive\n","    \n","    # Perform Box-Cox transformation\n","    df_copy[f'{column}_boxcox'], lambda_param = stats.boxcox(df_copy[column])\n","    \n","    print(f\"Transforming column: {column}\")\n","    print(f\"Optimal lambda for Box-Cox transformation: {lambda_param}\")\n","    print(f\"Number of rows before transformation: {len(df)}\")\n","    print(f\"Number of rows after removing NaN values: {len(df_copy)}\")\n","    \n","    return df_copy, lambda_param\n","\n","for column in skewed_columns:\n","    transformed_train_data, lambda_params[column] = box_cox_transform(train_data, column)\n","    # Update only the new transformed column in the original dataframe\n","    train_data[f'{column}_boxcox'] = transformed_train_data[f'{column}_boxcox']"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# Isolate the physical attribute columns and some contextual columns for analysis\n","physical_columns = [\n","    'Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical_Weight_X_PCIAT_Total',\n","    'CGAS-Season', 'CGAS-CGAS_Score_boxcox', 'Physical-Season', 'Physical-BMI',\n","    'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'BIA_Fat_X_Physical_Weight',\n","    'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'BIA_BMI_X_PCIAT_Total',\n","    'fitness_score', 'BIA-BIA_Frame_num', 'BIA-BIA_BMI', 'PreInt_EduHx-computerinternet_hoursday',\n","    'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total', 'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total',\n","    'BIA-BIA_Fat^1', 'BIA-BIA_Fat^2', 'BIA-BIA_Fat^3', 'Physical-Weight^1',\n","    'Physical-Weight^2', 'sii'\n","]\n","\n","# Isolate the fitness attributes\n","# Removed columns: 'FGC-FGC_CU' 'FGC-FGC_PU', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_SRL' \n","# 'FGC-FGC_GSND_Zone' 'FGC-FGC_GSND' 'FGC-FGC_GSD' 'FGC-FGC_GSD_Zone' 'Fitness_Endurance-Time_Sec'\n","fitness_columns = [\n","    'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Physical_Weight_X_PCIAT_Total',\n","    'FGC-Season', 'FGC-FGC_CU_Zone', 'FGC-FGC_SR', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL', 'BIA_Fat_X_Physical_Weight',\n","    'FGC-FGC_TL_Zone', 'FGC-FGC_GS', 'fitness_score', 'BIA-BIA_BMI', 'Physical-BMI', 'BIA_BMI_X_PCIAT_Total',\n","    'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'BIA-BIA_Fat^1', 'BIA-BIA_Fat^2', 'BIA-BIA_Fat^3',\n","    'Physical-Weight^1', 'Physical-Weight^2', 'sii'\n","]\n","\n","# Isolate the BIA attributes\n","bia_columns = [\n","    'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'BIA-BIA_BMC_boxcox', 'BIA_Fat_X_Physical_Weight',\n","    'BIA-BIA_BMI', 'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox', 'BIA-BIA_FFMI_boxcox',\n","    'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat_boxcox', 'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_TBW_boxcox',\n","    'fitness_score', 'Physical-BMI', 'PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'BIA_BMI_X_PCIAT_Total', 'Physical_Weight_X_PCIAT_Total',\n","    'BIA-BIA_Fat^1', 'BIA-BIA_Fat^2', 'BIA-BIA_Fat^3', 'Physical-Weight^1', 'Physical-Weight^2', 'sii'\n","]\n","\n","# Isolate the PAQ, PCIAT, and SDS\n","child_info_columns = [\n","    'PreInt_EduHx-computerinternet_hoursday', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total',\n","    'PAQ_Total', 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n","    'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'BIA_Fat_X_Physical_Weight',\n","    'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'BIA_BMI_X_PCIAT_Total',\n","    'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'Physical_Weight_X_PCIAT_Total',\n","    'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n","    'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'BIA-BIA_BMI', 'fitness_score', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical-BMI', 'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'BIA-BIA_Fat^1',\n","    'BIA-BIA_Fat^2', 'BIA-BIA_Fat^3', 'Physical-Weight^1', 'Physical-Weight^2', 'sii'\n","]\n","\n","# Isolate the Actigraphy data\n","# removed columns: 'stat_41', 'stat_42', 'stat_39','stat_92_boxcox' 'stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', 'stat_6', 'stat_7', \n","# 'stat_8', 'stat_9', 'stat_10','stat_11'\n","actigraphy_columns = [\n","    'combined_actigraphy_stat', 'stat_12', 'stat_13', 'stat_14', 'stat_15', 'stat_16', 'stat_17', 'stat_18', 'stat_19', 'stat_20',\n","    'stat_21', 'stat_22', 'stat_23_boxcox', 'stat_24', 'stat_25', 'stat_26', 'stat_27', 'stat_28', 'stat_29', 'stat_30',\n","    'stat_31', 'stat_32', 'stat_33', 'stat_34', 'stat_35_boxcox', 'stat_36', 'stat_37', 'stat_38_boxcox', 'stat_40_boxcox',\n","    'stat_43', 'stat_44', 'stat_45', 'stat_46', 'stat_47_boxcox', 'stat_48', 'stat_49', 'stat_50', 'BIA_Fat_X_Physical_Weight',\n","    'stat_51', 'stat_52', 'stat_53', 'stat_54_boxcox', 'stat_55', 'stat_56', 'stat_57', 'stat_58', 'stat_59', 'stat_60',\n","    'stat_61', 'stat_62', 'stat_63', 'stat_64', 'stat_65', 'stat_66_boxcox', 'stat_67', 'stat_68', 'stat_69', 'stat_70',\n","    'stat_71', 'stat_72', 'stat_73', 'stat_74', 'stat_75', 'stat_76', 'stat_77', 'stat_78_boxcox', 'stat_79', 'stat_80_boxcox',\n","    'stat_81', 'stat_82', 'stat_83', 'stat_84', 'stat_85', 'stat_86', 'stat_87', 'stat_88_boxcox', 'stat_89', 'stat_90_boxcox',\n","    'stat_91', 'stat_93', 'stat_94', 'stat_95', 'PreInt_EduHx-computerinternet_hoursday', 'CGAS_Score_X_PCIAT_Total', 'BIA_BMI_X_PCIAT_Total',\n","    'BIA-BIA_Frame_num', 'SDS-SDS_Total_T', 'BIA-BIA_BMI', 'Physical-BMI', 'BIA_Fat_X_PCIAT_Total', 'Physical_Weight_X_PCIAT_Total', 'sii'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to analyze columns\n","def analyze_column(column):\n","    total_count = len(train_data)\n","    missing_count = train_data[column].isnull().sum()\n","    missing_percentage = (missing_count / total_count) * 100\n","    unique_values = train_data[column].nunique()\n","    \n","    if pd.api.types.is_numeric_dtype(train_data[column]):\n","        mean_value = train_data[column].mean()\n","        median_value = train_data[column].median()\n","        std_dev = train_data[column].std()\n","        min_value = train_data[column].min()\n","        max_value = train_data[column].max()\n","        return {\n","            \"Column\": column,\n","            \"Total Count\": total_count,\n","            \"Missing Count\": missing_count,\n","            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n","            \"Unique Values\": unique_values,\n","            \"Data Type\": train_data[column].dtype,\n","            \"Mean\": mean_value,\n","            \"Median\": median_value,\n","            \"Standard Deviation\": std_dev,\n","            \"Minimum\": min_value,\n","            \"Maximum\": max_value\n","        }\n","    else:\n","        top_values = train_data[column].value_counts().head(3).to_dict()\n","        return {\n","            \"Column\": column,\n","            \"Total Count\": total_count,\n","            \"Missing Count\": missing_count,\n","            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n","            \"Unique Values\": unique_values,\n","            \"Data Type\": train_data[column].dtype,\n","            \"Top 3 Values\": top_values\n","        }\n","\n","# Physical column profiles        \n","physical_column_profiles = [analyze_column(col) for col in physical_columns]\n","physical_column_profiles_df = pd.DataFrame(physical_column_profiles)\n","\n","# Save column profiles to CSV\n","physical_column_profiles_df.to_csv(os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv')}\")\n","\n","# Fitness column profiles\n","fitness_column_profiles = [analyze_column(col) for col in fitness_columns]\n","fitness_column_profiles_df = pd.DataFrame(fitness_column_profiles)\n","\n","# Save column profiles to CSV\n","fitness_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'fitness_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'fitness_column_profiles.csv')}\")\n","\n","# BIA column profiles\n","bia_column_profiles = [analyze_column(col) for col in bia_columns]\n","bia_column_profiles_df = pd.DataFrame(bia_column_profiles)\n","\n","# Save column profiles to CSV\n","bia_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'bia_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'bia_column_profiles.csv')}\")\n","\n","# Child info column profiles\n","child_info_column_profiles = [analyze_column(col) for col in child_info_columns]\n","child_info_column_profiles_df = pd.DataFrame(child_info_column_profiles)\n","\n","# Save column profiles to CSV\n","child_info_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'child_info_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'child_info_column_profiles.csv')}\")\n","\n","# Actigraphy info column profiles\n","actigraphy_column_profiles = [analyze_column(col) for col in actigraphy_columns]\n","actigraphy_column_profiles_df = pd.DataFrame(actigraphy_column_profiles)\n","\n","# Save column profiles to CSV\n","actigraphy_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize missing data\n","# Physical columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[physical_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Physical Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png')}\")\n","\n","# Fitness columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[fitness_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Fitness Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png')}\")\n","\n","# BIA columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[bia_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in BIA Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png')}\")\n","\n","# Child info columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[child_info_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Child info Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png')}\")\n","\n","# Actigraphy info columns\n","plt.figure(figsize=(40, 6))\n","sns.heatmap(train_data[actigraphy_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Actigraphy info Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation matrix for physical numeric columns\n","physical_numeric_columns = train_data[physical_columns].select_dtypes(include=[np.number]).columns\n","physical_correlation_matrix = train_data[physical_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(physical_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Physical Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png'))\n","plt.close()\n","print(f\"Physical correlation matrix saved to {os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png')}\")\n","\n","# Correlation matrix for fitness numeric columns\n","fitness_numeric_columns = train_data[fitness_columns].select_dtypes(include=[np.number]).columns\n","fitness_correlation_matrix = train_data[fitness_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(fitness_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Fitness Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png'))\n","plt.close()\n","print(f\"Fitness correlation matrix saved to {os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png')}\")\n","\n","# Correlation matrix for bia numeric columns\n","bia_numeric_columns = train_data[bia_columns].select_dtypes(include=[np.number]).columns\n","bia_correlation_matrix = train_data[bia_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(bia_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric BIA Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(bia_analysis_output_folder, 'BIA_correlation_matrix.png')}\")\n","\n","# Correlation matrix for child info numeric columns\n","child_info_numeric_columns = train_data[child_info_columns].select_dtypes(include=[np.number]).columns\n","child_info_correlation_matrix = train_data[child_info_numeric_columns].corr()\n","\n","plt.figure(figsize=(24, 22))\n","sns.heatmap(child_info_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Child info Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png')}\")\n","\n","# Correlation matrix for actigraphy numeric columns\n","actigraphy_numeric_columns = train_data[actigraphy_columns].select_dtypes(include=[np.number]).columns\n","actigraphy_correlation_matrix = train_data[actigraphy_numeric_columns].corr()\n","\n","plt.figure(figsize=(80, 78))\n","sns.heatmap(actigraphy_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Actigraphy Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine all columns into a single list\n","all_columns = []\n","all_columns.extend(physical_numeric_columns)\n","all_columns.extend(fitness_numeric_columns)\n","all_columns.extend(bia_numeric_columns)\n","all_columns.extend(child_info_numeric_columns)\n","all_columns.extend(actigraphy_numeric_columns)\n","\n","# Create a tqdm progress bar\n","with tqdm(total=len(all_columns), desc=\"Creating distribution plots\") as pbar:\n","    # Distribution plots for physical numeric columns\n","    for column in physical_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of physical {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(physical_analysis_output_folder, f'physical_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","    # Distribution plots for fitness numeric columns\n","    for column in fitness_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of fitness {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(fitness_analysis_output_folder, f'fitness_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","    # Distribution plots for BIA numeric columns\n","    for column in bia_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of bia {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(bia_analysis_output_folder, f'bia_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","        \n","    # Distribution plots for Child info numeric columns\n","    for column in child_info_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of child info {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(child_info_analysis_output_folder, f'child_info_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","        \n","    # Distribution plots for actigraphy numeric columns\n","    for column in actigraphy_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of actigraphy {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(actigraphy_analysis_output_folder, f'actigraphy_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","print(\"All analyses completed and saved to the 'analysis_output' folder.\")"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Supplement missing data with data from WHO\n","# Load who data and group\n","def load_who_bmi_data(file_path):\n","    who_data = pd.read_csv(file_path)\n","    who_data = who_data.groupby(['age', 'sex']).agg({\n","        'L': 'mean', 'mean_bmi': 'mean', 'S': 'mean'\n","    }).reset_index()\n","    who_data = who_data.set_index(['sex', 'age'])\n","    return who_data\n","\n","def load_who_height_data(file_path):\n","    who_data = pd.read_csv(file_path)\n","    who_data = who_data.groupby(['age', 'sex']).agg({\n","        'mean_height': 'mean'\n","    }).reset_index()\n","    who_data = who_data.set_index(['sex', 'age'])\n","    return who_data\n","\n","who_bmi_data = load_who_bmi_data('supplemental_data/bmi_for_age_5_to_19.csv')\n","who_height_data = load_who_height_data('supplemental_data/height_for_age_5_to_19.csv')"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Defining functions to Impute with data from WHO\n","def get_who_stats(age, sex, data_type='bmi'):\n","    try:\n","        if data_type == 'bmi':\n","            stats = who_bmi_data.loc[(sex, age), ['mean_bmi', 'S']]\n","            return stats['mean_bmi'], stats['S']\n","        elif data_type == 'height':\n","            stats = who_height_data.loc[(sex, age), 'mean_height']\n","            return stats\n","    except KeyError:\n","        return None, None if data_type == 'bmi' else None\n","    \n","def impute_bmi(age, sex):\n","    mean_bmi, sd = get_who_stats(age, sex, 'bmi')\n","    if mean_bmi is not None and sd is not None:\n","        imputed_bmi = np.random.normal(mean_bmi, sd)\n","        return round(imputed_bmi, 2)\n","    else:\n","        return None\n","\n","def impute_height(age, sex):\n","    mean_height_cm = get_who_stats(age, sex, 'height')\n","    if mean_height_cm is not None:\n","        mean_height_inches = mean_height_cm / 2.54  # Convert cm to inches\n","        return round(mean_height_inches, 2)\n","    else:\n","        return None\n","    \n","def impute_weight(bmi, height_inches):\n","    if bmi is not None and height_inches is not None:\n","        height_meters = height_inches * 0.0254  # Convert inches to meters\n","        weight_kg = bmi * (height_meters ** 2)\n","        weight_lbs = weight_kg * 2.20462  # Convert kg to lbs\n","        return round(weight_lbs, 2)\n","    else:\n","        return None\n","    \n","def apply_imputation(df):\n","    def impute_if_missing(row):\n","        age = row['Basic_Demos-Age']\n","        sex = row['Basic_Demos-Sex']\n","        \n","        if pd.isna(row['Physical-BMI']) and 5 <= age <= 19:\n","            row['Physical-BMI'] = impute_bmi(age, sex)\n","        \n","        if pd.isna(row['Physical-Height']) and 5 <= age <= 19:\n","            row['Physical-Height'] = impute_height(age, sex)\n","        \n","        if pd.isna(row['Physical-Weight']) and row['Physical-BMI'] is not None and row['Physical-Height'] is not None:\n","            row['Physical-Weight'] = impute_weight(row['Physical-BMI'], row['Physical-Height'])\n","        \n","        return row\n","    \n","    return df.apply(impute_if_missing, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply to datasets\n","train_data = apply_imputation(train_data)\n","test_data = apply_imputation(test_data)\n","\n","# Check the results\n","print(\"Number of missing values after imputation:\")\n","print(\"BMI:\", train_data['Physical-BMI'].isna().sum())\n","print(\"Height:\", train_data['Physical-Height'].isna().sum())\n","print(\"Weight:\", train_data['Physical-Weight'].isna().sum())\n","\n","print(\"\\nSample of imputed BMI, Height, and Weight values:\")\n","imputed_sample = train_data[\n","    (train_data['Physical-BMI'].notnull() | train_data['Physical-Height'].notnull() | train_data['Physical-Weight'].notnull()) &\n","    ((train_data['Physical-BMI'].notnull() != train_data['Physical-BMI'].notnull().shift()) |\n","     (train_data['Physical-Height'].notnull() != train_data['Physical-Height'].notnull().shift()) |\n","     (train_data['Physical-Weight'].notnull() != train_data['Physical-Weight'].notnull().shift()))\n","].sample(5)[['Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical-BMI', 'Physical-Height', 'Physical-Weight']]\n","print(imputed_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional analysis of imputation results\n","print(\"\\nImputation summary by age:\")\n","age_summary = train_data.groupby('Basic_Demos-Age').agg({\n","    'Physical-BMI': ['count', 'mean', 'std', 'min', 'max'],\n","    'Physical-Height': ['count', 'mean', 'std', 'min', 'max'],\n","    'Physical-Weight': ['count', 'mean', 'std', 'min', 'max'],\n","    'Basic_Demos-Sex': 'count'\n","})\n","print(age_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot of imputed vs. original data\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-BMI', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('BMI vs Age (After Imputation)')\n","plt.savefig('analysis_output/bmi_vs_age_imputed.png')\n","plt.close()\n","\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-Height', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('Height vs Age (After Imputation)')\n","plt.savefig('analysis_output/height_vs_age_imputed.png')\n","plt.close()\n","\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-Weight', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('Weight vs Age (After Imputation)')\n","plt.savefig('analysis_output/weight_vs_age_imputed.png')\n","plt.close()\n","\n","print(\"Imputation analysis plots saved in the 'analysis_output' folder.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Export train_data to CSV\n","train_output_path = os.path.join(output_folder, 'train_data_imputed.csv')\n","train_data.to_csv(train_output_path, index=False)\n","print(f\"Imputed train data exported to: {train_output_path}\")\n","\n","# Export test_data to CSV\n","test_output_path = os.path.join(output_folder, 'test_data_imputed.csv')\n","test_data.to_csv(test_output_path, index=False)\n","print(f\"Imputed test data exported to: {test_output_path}\")\n","\n","print(\"Data export completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define categorical columns\n","category_columns =[\n","    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season',\n","    'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season',\n","    'PreInt_EduHx-Season' \n","]\n","\n","# Enumerate categorical columns\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in category_columns:\n","    mapping = create_mapping(col, train_data)\n","    train_data[col] = train_data[col].replace(mapping).infer_objects(copy=False).astype(int)\n","    test_data[col] = test_data[col].replace(create_mapping(col, test_data)).infer_objects(copy=False).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# High-risk vs. Low-risk analysis\n","# Suppress RuntimeWarnings\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","\n","# List of columns to drop\n","columns_to_drop = [\n","    'FGC-FGC_CU', 'FGC-FGC_PU', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', \n","    'FGC-FGC_SRL', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', \n","    'Fitness_Endurance-Time_Sec', 'stat_41', 'stat_42', 'stat_39', 'stat_92_boxcox', 'stat_92',\n","    'stat_44'\n","]\n","\n","train_data = train_data.drop(columns=columns_to_drop, errors='ignore')\n","\n","# Separate high-risk and low-risk groups\n","high_risk = train_data[train_data['sii'] == 3]\n","low_risk = train_data[train_data['sii'] != 3]\n","\n","# Function to calculate summary statistics\n","def get_summary_stats(group):\n","    return pd.DataFrame({\n","        'mean': group.mean(),\n","        'median': group.median(),\n","        'std': group.std(),\n","        'min': group.min(),\n","        'max': group.max()\n","    })\n","\n","# List of columns to analyze (excluding 'sii' and non-numeric columns)\n","columns_to_analyze = train_data.select_dtypes(include=[np.number]).columns.drop('sii')\n","\n","# Calculate summary statistics for both groups\n","high_risk_stats = get_summary_stats(high_risk[columns_to_analyze])\n","low_risk_stats = get_summary_stats(low_risk[columns_to_analyze])\n","\n","# Calculate the difference in means\n","mean_diff = high_risk_stats['mean'] - low_risk_stats['mean']\n","\n","# Perform statistical tests to check for significant differences\n","def perform_statistical_test(col):\n","    try:\n","        # Try t-test first\n","        t_stat, p_val = stats.ttest_ind(high_risk[col].dropna(), low_risk[col].dropna(), equal_var=False)\n","        test_type = 't-test'\n","    except Exception:\n","        # If t-test fails, use Mann-Whitney U test\n","        try:\n","            u_stat, p_val = stats.mannwhitneyu(high_risk[col].dropna(), low_risk[col].dropna(), alternative='two-sided')\n","            test_type = 'Mann-Whitney U'\n","        except Exception:\n","            # If both tests fail, return NaN values\n","            return pd.Series({'statistic': np.nan, 'p_value': np.nan, 'test_type': 'Failed'})\n","    \n","    return pd.Series({'statistic': t_stat if test_type == 't-test' else u_stat, \n","                      'p_value': p_val, \n","                      'test_type': test_type})\n","\n","test_results = pd.DataFrame({col: perform_statistical_test(col) for col in columns_to_analyze}).T\n","\n","# Combine results\n","comparison_results = pd.concat([\n","    high_risk_stats.add_prefix('high_risk_'),\n","    low_risk_stats.add_prefix('low_risk_'),\n","    mean_diff.rename('mean_difference'),\n","    test_results\n","], axis=1)\n","\n","# Sort by absolute mean difference\n","comparison_results = comparison_results.sort_values('mean_difference', key=abs, ascending=False)\n","\n","# Display the top 20 most different features\n","print(comparison_results.head(20))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["# Visualize distributions for top features\n","def plot_distribution(feature):\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data=train_data, x=feature, hue='sii', kde=True, common_norm=False)\n","    plt.title(f'Distribution of {feature} by Risk Group')\n","    plt.savefig(f'analysis_output/distribution_{feature}.png')\n","    plt.close()\n","\n","for feature in comparison_results.head(20).index:\n","    plot_distribution(feature)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation analysis\n","def get_correlations(group):\n","    return group.corr().iloc[:, 0].sort_values(key=abs, ascending=False)\n","\n","high_risk_corr = get_correlations(high_risk[columns_to_analyze])\n","low_risk_corr = get_correlations(low_risk[columns_to_analyze])\n","\n","print(\"\\nTop correlations in high-risk group:\")\n","print(high_risk_corr.head(20))\n","print(\"\\nTop correlations in low-risk group:\")\n","print(low_risk_corr.head(20))\n","\n","# Calculate correlation differences\n","corr_diff = high_risk_corr - low_risk_corr\n","corr_diff = corr_diff.sort_values(key=abs, ascending=False)\n","\n","print(\"\\nTop correlation differences (high-risk minus low-risk):\")\n","print(corr_diff.head(20))\n","\n","# Save correlation results to CSV\n","pd.DataFrame({\n","    'high_risk_corr': high_risk_corr,\n","    'low_risk_corr': low_risk_corr,\n","    'correlation_difference': corr_diff\n","}).to_csv('analysis_output/correlation_comparison.csv')\n","\n","# Save results to CSV\n","comparison_results.to_csv('analysis_output/high_low_risk_comparison.csv')\n","\n","print(\"\\nAnalysis complete. Results saved to 'analysis_output/high_low_risk_comparison.csv' and 'analysis_output/correlation_comparison.csv'\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize top correlation differences\n","plt.figure(figsize=(16, 12))\n","corr_diff.head(20).plot(kind='bar')\n","plt.title('Top 20 Correlation Differences (High-risk minus Low-risk)')\n","plt.xlabel('Features')\n","plt.ylabel('Correlation Difference')\n","plt.tight_layout()\n","plt.savefig('analysis_output/top_correlation_differences.png')\n","plt.close()\n","\n","print(\"Correlation difference plot saved to 'analysis_output/top_correlation_differences.png'\")"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.7"}},"nbformat":4,"nbformat_minor":2}
