{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import random\n","from sklearn.base import clone\n","from sklearn.metrics import cohen_kappa_score\n","from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\n","from sklearn.feature_selection import SelectKBest, f_classif\n","from sklearn.ensemble import VotingRegressor, RandomForestClassifier\n","from sklearn.impute import SimpleImputer, KNNImputer\n","from sklearn.compose import ColumnTransformer\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import PolynomialFeatures, LabelEncoder, StandardScaler\n","from scipy.optimize import minimize, minimize_scalar\n","from scipy import stats\n","from concurrent.futures import ThreadPoolExecutor\n","from tqdm import tqdm\n","from colorama import Fore, Style\n","from IPython.display import clear_output\n","from lightgbm import LGBMRegressor, LGBMClassifier\n","from xgboost import XGBRegressor, XGBClassifier\n","from catboost import CatBoostRegressor, CatBoostClassifier\n","from imblearn.over_sampling import SMOTE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import warnings"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Create output folders\n","output_folder = 'output'\n","if not os.path.exists(output_folder):\n","    os.makedirs(output_folder)\n","\n","# Create separate analysis output folders\n","analysis_output_folder = 'analysis_output'\n","os.makedirs(analysis_output_folder, exist_ok=True)\n","\n","physical_analysis_output_folder = 'analysis_output/physical'\n","os.makedirs(physical_analysis_output_folder, exist_ok=True)\n","\n","fitness_analysis_output_folder = 'analysis_output/fitness'\n","os.makedirs(fitness_analysis_output_folder, exist_ok=True)\n","\n","bia_analysis_output_folder = 'analysis_output/bia'\n","os.makedirs(bia_analysis_output_folder, exist_ok=True)\n","\n","child_info_analysis_output_folder = 'analysis_output/child_info'\n","os.makedirs(child_info_analysis_output_folder, exist_ok=True)\n","\n","actigraphy_analysis_output_folder = 'analysis_output/actigraphy'\n","os.makedirs(actigraphy_analysis_output_folder, exist_ok=True)\n","\n","# Load data functions\n","def process_file(filename, dirname):\n","    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n","    df.drop('step', axis=1, inplace=True)\n","    return df.describe().values.reshape(-1), filename.split('=')[1]\n","\n","def load_time_series(dirname) -> pd.DataFrame:\n","    ids = os.listdir(dirname)\n","    with ThreadPoolExecutor() as executor:\n","        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n","    stats, indexes = zip(*results)\n","    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n","    df['id'] = indexes\n","    return df\n","\n","# Set display all columns in dataframes property\n","pd.options.display.max_columns = None"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load data\n","train_data = pd.read_csv('input/train.csv')\n","test_data = pd.read_csv('input/test.csv')\n","sample_data = pd.read_csv('input/sample_submission.csv')\n","\n","train_ts_data = load_time_series(\"input/series_train.parquet\")\n","test_ts_data = load_time_series(\"input/series_test.parquet\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Remove id column from time series data\n","time_series_columns = train_ts_data.columns.tolist()\n","time_series_columns.remove(\"id\")\n","\n","# Merge data\n","train_data = pd.merge(train_data, train_ts_data, how=\"left\", on='id')\n","test_data = pd.merge(test_data, test_ts_data, how=\"left\", on='id')\n","train_data = train_data.drop('id', axis=1)\n","test_data = test_data.drop('id', axis=1)\n","\n","train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Skew removal for some BIA columns\n","skewed_columns = [\n","    'BIA-BIA_BMC', 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_Fat',\n","    'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', \n","    'BIA-BIA_TBW', 'CGAS-CGAS_Score', 'stat_23', 'stat_35', 'stat_38', 'stat_40', 'stat_47',\n","    'stat_54', 'stat_66', 'stat_78', 'stat_80', 'stat_88', 'stat_90'\n","]\n","lambda_params = {}\n","\n","# Define the box-cox function to remove skew\n","def box_cox_transform(df, column, lambda_param=None):\n","    # Create a copy of the dataframe\n","    df_copy = df.copy()\n","    \n","    # Drop NaN values for the specific column\n","    df_copy = df_copy.dropna(subset=[column])\n","    \n","    # Ensure all values are positive\n","    min_value = df_copy[column].min()\n","    if min_value <= 0:\n","        df_copy[column] = df_copy[column] - min_value + 1  # Add 1 to ensure all values are positive\n","    \n","    # Perform Box-Cox transformation\n","    if lambda_param is None:\n","        df_copy[f'{column}_boxcox'], lambda_param = stats.boxcox(df_copy[column])\n","        print(f\"Transforming column: {column}\")\n","        print(f\"Optimal lambda for Box-Cox transformation: {lambda_param}\")\n","    else:\n","        df_copy[f'{column}_boxcox'] = stats.boxcox(df_copy[column], lmbda=lambda_param)\n","        print(f\"Applying transformation to column: {column} with lambda: {lambda_param}\")\n","    \n","    print(f\"Number of rows before transformation: {len(df)}\")\n","    print(f\"Number of rows after removing NaN values: {len(df_copy)}\")\n","    \n","    return df_copy, lambda_param\n","\n","# Apply Box-Cox transformation to train data and store lambda values\n","for column in skewed_columns:\n","    transformed_train_data, lambda_params[column] = box_cox_transform(train_data, column)\n","    # Update only the new transformed column in the original dataframe\n","    train_data[f'{column}_boxcox'] = transformed_train_data[f'{column}_boxcox']\n","\n","# Apply the same transformation to test data using stored lambda values\n","for column in skewed_columns:\n","    transformed_test_data, _ = box_cox_transform(test_data, column, lambda_param=lambda_params[column])\n","    # Update only the new transformed column in the original dataframe\n","    test_data[f'{column}_boxcox'] = transformed_test_data[f'{column}_boxcox']\n","\n","# Function to handle infinite values\n","def replace_inf_with_max(df):\n","    for column in df.columns:\n","        if df[column].dtype == 'float64':\n","            max_value = df[column][~np.isinf(df[column])].max()\n","            df[column] = df[column].replace([np.inf, -np.inf], max_value)\n","    return df\n","\n","# Replace infinite values with the maximum non-infinite value in each column\n","train_data = replace_inf_with_max(train_data)\n","test_data = replace_inf_with_max(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Feature engineering\n","def engineer_features(df, is_train=True):\n","    # Create a copy of the dataframe to avoid modifying the original\n","    df = df.copy()\n","    \n","    # Combine all grip strength\n","    df['FGC-FGC_GS'] = df['FGC-FGC_GSD_Zone'] + df['FGC-FGC_GSND_Zone']\n","\n","    # Combine all sit and reach\n","    df['FGC-FGC_SR'] = df['FGC-FGC_SRL_Zone'] + df['FGC-FGC_SRR_Zone']\n","\n","    # Create a fitness score by adding the zone fitness data\n","    df['fitness_score'] = df['FGC-FGC_GS'] + df['FGC-FGC_SR'] + df['FGC-FGC_CU_Zone'] + df['FGC-FGC_PU_Zone'] + df['FGC-FGC_TL_Zone']\n","\n","    # Combine PAQ_A-PAQ_A_Total and PAQ_C-PAQ_C_Total into one column\n","    df['PAQ_Total'] = df['PAQ_A-PAQ_A_Total'].combine_first(df['PAQ_C-PAQ_C_Total'])\n","\n","    # Create interaction features\n","    interaction_features = [\n","        ('BIA-BIA_Fat', 'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total'),\n","        ('Physical-Weight', 'BIA-BIA_BMI', 'Physical_Weight_X_BIA_BMI'),\n","        ('CGAS-CGAS_Score', 'PCIAT-PCIAT_Total', 'CGAS_Score_X_PCIAT_Total'),\n","        ('BIA-BIA_Fat', 'Physical-Weight', 'BIA_Fat_X_Physical_Weight'),\n","        ('BIA-BIA_BMI', 'PCIAT-PCIAT_Total', 'BIA_BMI_X_PCIAT_Total'),\n","        ('Physical-Weight', 'PCIAT-PCIAT_Total', 'Physical_Weight_X_PCIAT_Total')\n","    ]\n","\n","    for feature1, feature2, new_feature in interaction_features:\n","        if is_train or (feature2 != 'PCIAT-PCIAT_Total' and feature1 != 'PCIAT-PCIAT_Total'):\n","            df[new_feature] = df[feature1] * df[feature2]\n","        else:\n","            # For test set, create a placeholder column filled with NaN\n","            df[new_feature] = np.nan\n","            print(f\"Warning: {new_feature} could not be created for test set due to missing 'PCIAT-PCIAT_Total'\")\n","\n","    # Combine identical actigraphy stats\n","    columns_to_combine = ['stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', \n","                          'stat_6', 'stat_7', 'stat_8', 'stat_9', 'stat_10', 'stat_11']\n","    \n","    # Check if columns are identical\n","    is_identical = df[columns_to_combine].nunique().eq(1).all()\n","    \n","    if is_identical:\n","        # If they are identical, we can just take the first column and rename it\n","        df['combined_actigraphy_stat'] = df[columns_to_combine[0]]\n","        print(f\"Columns {columns_to_combine} have been combined into combined_actigraphy_stat\")\n","    else:\n","        # If not exactly identical, take the mean\n","        print(\"Warning: The actigraphy stat columns are not identical. Taking the average instead.\")\n","        df['combined_actigraphy_stat'] = df[columns_to_combine].mean(axis=1)\n","    \n","    # Drop the original columns\n","    df = df.drop(columns=columns_to_combine)\n","\n","    return df\n","\n","# Apply feature engineering to train data\n","train_data = engineer_features(train_data, is_train=True)\n","\n","# Apply feature engineering to test data\n","test_data = engineer_features(test_data, is_train=False)\n","\n","print(\"Feature engineering completed for both train and test data.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# Isolate the physical attribute columns and some contextual columns for analysis\n","physical_columns = [\n","    'Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical_Weight_X_PCIAT_Total',\n","    'CGAS-Season', 'CGAS-CGAS_Score_boxcox', 'Physical-Season', 'Physical-BMI',\n","    'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference', 'BIA_Fat_X_Physical_Weight',\n","    'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'BIA_BMI_X_PCIAT_Total',\n","    'fitness_score', 'BIA-BIA_Frame_num', 'BIA-BIA_BMI', 'PreInt_EduHx-computerinternet_hoursday',\n","    'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total', 'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total',\n","    'sii'\n","]\n","\n","# Isolate the fitness attributes\n","# Removed columns: 'FGC-FGC_CU' 'FGC-FGC_PU', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', 'FGC-FGC_SRL' \n","# 'FGC-FGC_GSND_Zone' 'FGC-FGC_GSND' 'FGC-FGC_GSD' 'FGC-FGC_GSD_Zone' 'Fitness_Endurance-Time_Sec'\n","fitness_columns = [\n","    'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Physical_Weight_X_PCIAT_Total',\n","    'FGC-Season', 'FGC-FGC_CU_Zone', 'FGC-FGC_SR', 'FGC-FGC_PU_Zone', 'FGC-FGC_TL', 'BIA_Fat_X_Physical_Weight',\n","    'FGC-FGC_TL_Zone', 'FGC-FGC_GS', 'fitness_score', 'BIA-BIA_BMI', 'Physical-BMI', 'BIA_BMI_X_PCIAT_Total',\n","    'BIA-BIA_Frame_num', 'PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'sii'\n","]\n","\n","# Isolate the BIA attributes\n","bia_columns = [\n","    'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_Frame_num', 'BIA-BIA_BMC_boxcox', 'BIA_Fat_X_Physical_Weight',\n","    'BIA-BIA_BMI', 'BIA-BIA_BMR_boxcox', 'BIA-BIA_DEE_boxcox', 'BIA-BIA_ECW_boxcox', 'BIA-BIA_FFM_boxcox', 'BIA-BIA_FFMI_boxcox',\n","    'BIA-BIA_FMI_boxcox', 'BIA-BIA_Fat_boxcox', 'BIA-BIA_ICW_boxcox', 'BIA-BIA_LDM_boxcox', 'BIA-BIA_LST_boxcox', 'BIA-BIA_TBW_boxcox',\n","    'fitness_score', 'Physical-BMI', 'PreInt_EduHx-computerinternet_hoursday', 'PCIAT-PCIAT_Total', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'BIA_BMI_X_PCIAT_Total', 'Physical_Weight_X_PCIAT_Total',\n","    'sii'\n","]\n","\n","# Isolate the PAQ, PCIAT, and SDS\n","child_info_columns = [\n","    'PreInt_EduHx-computerinternet_hoursday', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-PAQ_C_Total',\n","    'PAQ_Total', 'PCIAT-Season', 'PCIAT-PCIAT_01', 'PCIAT-PCIAT_02', 'PCIAT-PCIAT_03', 'PCIAT-PCIAT_04',\n","    'PCIAT-PCIAT_05', 'PCIAT-PCIAT_06', 'PCIAT-PCIAT_07', 'PCIAT-PCIAT_08', 'PCIAT-PCIAT_09', 'BIA_Fat_X_Physical_Weight',\n","    'PCIAT-PCIAT_10', 'PCIAT-PCIAT_11', 'PCIAT-PCIAT_12', 'PCIAT-PCIAT_13', 'PCIAT-PCIAT_14', 'BIA_BMI_X_PCIAT_Total',\n","    'PCIAT-PCIAT_15', 'PCIAT-PCIAT_16', 'PCIAT-PCIAT_17', 'PCIAT-PCIAT_18', 'PCIAT-PCIAT_19', 'Physical_Weight_X_PCIAT_Total',\n","    'PCIAT-PCIAT_20', 'PCIAT-PCIAT_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n","    'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'BIA-BIA_BMI', 'fitness_score', 'BIA_Fat_X_PCIAT_Total',\n","    'Physical-BMI', 'Physical_Weight_X_BIA_BMI', 'CGAS_Score_X_PCIAT_Total', 'sii'\n","]\n","\n","# Isolate the Actigraphy data\n","# removed columns: 'stat_41', 'stat_42', 'stat_39','stat_92_boxcox' 'stat_0', 'stat_1', 'stat_2', 'stat_3', 'stat_4', 'stat_5', 'stat_6', 'stat_7', \n","# 'stat_8', 'stat_9', 'stat_10','stat_11'\n","actigraphy_columns = [\n","    'combined_actigraphy_stat', 'stat_12', 'stat_13', 'stat_14', 'stat_15', 'stat_16', 'stat_17', 'stat_18', 'stat_19', 'stat_20',\n","    'stat_21', 'stat_22', 'stat_23_boxcox', 'stat_24', 'stat_25', 'stat_26', 'stat_27', 'stat_28', 'stat_29', 'stat_30',\n","    'stat_31', 'stat_32', 'stat_33', 'stat_34', 'stat_35_boxcox', 'stat_36', 'stat_37', 'stat_38_boxcox', 'stat_40_boxcox',\n","    'stat_43', 'stat_44', 'stat_45', 'stat_46', 'stat_47_boxcox', 'stat_48', 'stat_49', 'stat_50', 'BIA_Fat_X_Physical_Weight',\n","    'stat_51', 'stat_52', 'stat_53', 'stat_54_boxcox', 'stat_55', 'stat_56', 'stat_57', 'stat_58', 'stat_59', 'stat_60',\n","    'stat_61', 'stat_62', 'stat_63', 'stat_64', 'stat_65', 'stat_66_boxcox', 'stat_67', 'stat_68', 'stat_69', 'stat_70',\n","    'stat_71', 'stat_72', 'stat_73', 'stat_74', 'stat_75', 'stat_76', 'stat_77', 'stat_78_boxcox', 'stat_79', 'stat_80_boxcox',\n","    'stat_81', 'stat_82', 'stat_83', 'stat_84', 'stat_85', 'stat_86', 'stat_87', 'stat_88_boxcox', 'stat_89', 'stat_90_boxcox',\n","    'stat_91', 'stat_93', 'stat_94', 'stat_95', 'PreInt_EduHx-computerinternet_hoursday', 'CGAS_Score_X_PCIAT_Total', 'BIA_BMI_X_PCIAT_Total',\n","    'BIA-BIA_Frame_num', 'SDS-SDS_Total_T', 'BIA-BIA_BMI', 'Physical-BMI', 'BIA_Fat_X_PCIAT_Total', 'Physical_Weight_X_PCIAT_Total', 'sii'\n","]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to analyze columns\n","def analyze_column(column):\n","    total_count = len(train_data)\n","    missing_count = train_data[column].isnull().sum()\n","    missing_percentage = (missing_count / total_count) * 100\n","    unique_values = train_data[column].nunique()\n","    \n","    if pd.api.types.is_numeric_dtype(train_data[column]):\n","        mean_value = train_data[column].mean()\n","        median_value = train_data[column].median()\n","        std_dev = train_data[column].std()\n","        min_value = train_data[column].min()\n","        max_value = train_data[column].max()\n","        return {\n","            \"Column\": column,\n","            \"Total Count\": total_count,\n","            \"Missing Count\": missing_count,\n","            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n","            \"Unique Values\": unique_values,\n","            \"Data Type\": train_data[column].dtype,\n","            \"Mean\": mean_value,\n","            \"Median\": median_value,\n","            \"Standard Deviation\": std_dev,\n","            \"Minimum\": min_value,\n","            \"Maximum\": max_value\n","        }\n","    else:\n","        top_values = train_data[column].value_counts().head(3).to_dict()\n","        return {\n","            \"Column\": column,\n","            \"Total Count\": total_count,\n","            \"Missing Count\": missing_count,\n","            \"Missing Percentage\": f\"{missing_percentage:.2f}%\",\n","            \"Unique Values\": unique_values,\n","            \"Data Type\": train_data[column].dtype,\n","            \"Top 3 Values\": top_values\n","        }\n","\n","# Physical column profiles        \n","physical_column_profiles = [analyze_column(col) for col in physical_columns]\n","physical_column_profiles_df = pd.DataFrame(physical_column_profiles)\n","\n","# Save column profiles to CSV\n","physical_column_profiles_df.to_csv(os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(physical_analysis_output_folder, 'physical_column_profiles.csv')}\")\n","\n","# Fitness column profiles\n","fitness_column_profiles = [analyze_column(col) for col in fitness_columns]\n","fitness_column_profiles_df = pd.DataFrame(fitness_column_profiles)\n","\n","# Save column profiles to CSV\n","fitness_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'fitness_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'fitness_column_profiles.csv')}\")\n","\n","# BIA column profiles\n","bia_column_profiles = [analyze_column(col) for col in bia_columns]\n","bia_column_profiles_df = pd.DataFrame(bia_column_profiles)\n","\n","# Save column profiles to CSV\n","bia_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'bia_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'bia_column_profiles.csv')}\")\n","\n","# Child info column profiles\n","child_info_column_profiles = [analyze_column(col) for col in child_info_columns]\n","child_info_column_profiles_df = pd.DataFrame(child_info_column_profiles)\n","\n","# Save column profiles to CSV\n","child_info_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'child_info_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'child_info_column_profiles.csv')}\")\n","\n","# Actigraphy info column profiles\n","actigraphy_column_profiles = [analyze_column(col) for col in actigraphy_columns]\n","actigraphy_column_profiles_df = pd.DataFrame(actigraphy_column_profiles)\n","\n","# Save column profiles to CSV\n","actigraphy_column_profiles_df.to_csv(os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv'), index=False)\n","print(f\"Column profiles saved to {os.path.join(analysis_output_folder, 'actigraphy_column_profiles.csv')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize missing data\n","# Physical columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[physical_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Physical Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(physical_analysis_output_folder, 'physical_missing_data_heatmap.png')}\")\n","\n","# Fitness columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[fitness_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Fitness Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(fitness_analysis_output_folder, 'fitness_missing_data_heatmap.png')}\")\n","\n","# BIA columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[bia_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in BIA Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(bia_analysis_output_folder, 'bia_missing_data_heatmap.png')}\")\n","\n","# Child info columns\n","plt.figure(figsize=(12, 6))\n","sns.heatmap(train_data[child_info_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Child info Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(child_info_analysis_output_folder, 'child_info_missing_data_heatmap.png')}\")\n","\n","# Actigraphy info columns\n","plt.figure(figsize=(40, 6))\n","sns.heatmap(train_data[actigraphy_columns].isnull(), cbar=False, yticklabels=False, cmap='viridis')\n","plt.title('Missing Data in Actigraphy info Attribute Columns')\n","plt.xlabel('Columns')\n","plt.ylabel('Rows')\n","plt.tight_layout()\n","plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png'))\n","plt.close()\n","print(f\"Missing data heatmap saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_missing_data_heatmap.png')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation matrix for physical numeric columns\n","physical_numeric_columns = train_data[physical_columns].select_dtypes(include=[np.number]).columns\n","physical_correlation_matrix = train_data[physical_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(physical_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Physical Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png'))\n","plt.close()\n","print(f\"Physical correlation matrix saved to {os.path.join(physical_analysis_output_folder, 'physical_correlation_matrix.png')}\")\n","\n","# Correlation matrix for fitness numeric columns\n","fitness_numeric_columns = train_data[fitness_columns].select_dtypes(include=[np.number]).columns\n","fitness_correlation_matrix = train_data[fitness_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(fitness_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Fitness Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png'))\n","plt.close()\n","print(f\"Fitness correlation matrix saved to {os.path.join(fitness_analysis_output_folder, 'fitness_correlation_matrix.png')}\")\n","\n","# Correlation matrix for bia numeric columns\n","bia_numeric_columns = train_data[bia_columns].select_dtypes(include=[np.number]).columns\n","bia_correlation_matrix = train_data[bia_numeric_columns].corr()\n","\n","plt.figure(figsize=(20, 18))\n","sns.heatmap(bia_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric BIA Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(bia_analysis_output_folder, 'bia_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(bia_analysis_output_folder, 'BIA_correlation_matrix.png')}\")\n","\n","# Correlation matrix for child info numeric columns\n","child_info_numeric_columns = train_data[child_info_columns].select_dtypes(include=[np.number]).columns\n","child_info_correlation_matrix = train_data[child_info_numeric_columns].corr()\n","\n","plt.figure(figsize=(24, 22))\n","sns.heatmap(child_info_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Child info Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(child_info_analysis_output_folder, 'child_info_correlation_matrix.png')}\")\n","\n","# Correlation matrix for actigraphy numeric columns\n","actigraphy_numeric_columns = train_data[actigraphy_columns].select_dtypes(include=[np.number]).columns\n","actigraphy_correlation_matrix = train_data[actigraphy_numeric_columns].corr()\n","\n","plt.figure(figsize=(80, 78))\n","sns.heatmap(actigraphy_correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)\n","plt.title('Correlation Matrix of Numeric Actigraphy Attributes')\n","plt.tight_layout()\n","plt.savefig(os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png'))\n","plt.close()\n","print(f\"BIA correlation matrix saved to {os.path.join(actigraphy_analysis_output_folder, 'actigraphy_correlation_matrix.png')}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Combine all columns into a single list\n","all_columns = []\n","all_columns.extend(physical_numeric_columns)\n","all_columns.extend(fitness_numeric_columns)\n","all_columns.extend(bia_numeric_columns)\n","all_columns.extend(child_info_numeric_columns)\n","all_columns.extend(actigraphy_numeric_columns)\n","\n","# Create a tqdm progress bar\n","with tqdm(total=len(all_columns), desc=\"Creating distribution plots\") as pbar:\n","    # Distribution plots for physical numeric columns\n","    for column in physical_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of physical {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(physical_analysis_output_folder, f'physical_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","    # Distribution plots for fitness numeric columns\n","    for column in fitness_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of fitness {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(fitness_analysis_output_folder, f'fitness_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","    # Distribution plots for BIA numeric columns\n","    for column in bia_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of bia {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(bia_analysis_output_folder, f'bia_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","        \n","    # Distribution plots for Child info numeric columns\n","    for column in child_info_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of child info {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(child_info_analysis_output_folder, f'child_info_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","        \n","    # Distribution plots for actigraphy numeric columns\n","    for column in actigraphy_numeric_columns:\n","        plt.figure(figsize=(10, 6))\n","        sns.histplot(train_data[column].dropna(), kde=True)\n","        plt.title(f'Distribution of actigraphy {column}')\n","        plt.xlabel(column)\n","        plt.ylabel('Count')\n","        plt.tight_layout()\n","        plt.savefig(os.path.join(actigraphy_analysis_output_folder, f'actigraphy_{column}_distribution.png'))\n","        plt.close()\n","        pbar.update(1)\n","\n","print(\"All analyses completed and saved to the 'analysis_output' folder.\")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["# Supplement missing data with data from WHO\n","# Load who data and group\n","def load_who_bmi_data(file_path):\n","    who_data = pd.read_csv(file_path)\n","    who_data = who_data.groupby(['age', 'sex']).agg({\n","        'L': 'mean', 'mean_bmi': 'mean', 'S': 'mean'\n","    }).reset_index()\n","    who_data = who_data.set_index(['sex', 'age'])\n","    return who_data\n","\n","def load_who_height_data(file_path):\n","    who_data = pd.read_csv(file_path)\n","    who_data = who_data.groupby(['age', 'sex']).agg({\n","        'mean_height': 'mean'\n","    }).reset_index()\n","    who_data = who_data.set_index(['sex', 'age'])\n","    return who_data\n","\n","who_bmi_data = load_who_bmi_data('supplemental_data/bmi_for_age_5_to_19.csv')\n","who_height_data = load_who_height_data('supplemental_data/height_for_age_5_to_19.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["# Defining functions to Impute with data from WHO\n","def get_who_stats(age, sex, data_type='bmi'):\n","    try:\n","        if data_type == 'bmi':\n","            stats = who_bmi_data.loc[(sex, age), ['mean_bmi', 'S']]\n","            return stats['mean_bmi'], stats['S']\n","        elif data_type == 'height':\n","            stats = who_height_data.loc[(sex, age), 'mean_height']\n","            return stats\n","    except KeyError:\n","        return None, None if data_type == 'bmi' else None\n","    \n","def impute_bmi(age, sex):\n","    mean_bmi, sd = get_who_stats(age, sex, 'bmi')\n","    if mean_bmi is not None and sd is not None:\n","        imputed_bmi = np.random.normal(mean_bmi, sd)\n","        return round(imputed_bmi, 2)\n","    else:\n","        return None\n","\n","def impute_height(age, sex):\n","    mean_height_cm = get_who_stats(age, sex, 'height')\n","    if mean_height_cm is not None:\n","        mean_height_inches = mean_height_cm / 2.54  # Convert cm to inches\n","        return round(mean_height_inches, 2)\n","    else:\n","        return None\n","    \n","def impute_weight(bmi, height_inches):\n","    if bmi is not None and height_inches is not None:\n","        height_meters = height_inches * 0.0254  # Convert inches to meters\n","        weight_kg = bmi * (height_meters ** 2)\n","        weight_lbs = weight_kg * 2.20462  # Convert kg to lbs\n","        return round(weight_lbs, 2)\n","    else:\n","        return None\n","    \n","def apply_imputation(df):\n","    def impute_if_missing(row):\n","        age = row['Basic_Demos-Age']\n","        sex = row['Basic_Demos-Sex']\n","        \n","        if pd.isna(row['Physical-BMI']) and 5 <= age <= 19:\n","            row['Physical-BMI'] = impute_bmi(age, sex)\n","        \n","        if pd.isna(row['Physical-Height']) and 5 <= age <= 19:\n","            row['Physical-Height'] = impute_height(age, sex)\n","        \n","        if pd.isna(row['Physical-Weight']) and row['Physical-BMI'] is not None and row['Physical-Height'] is not None:\n","            row['Physical-Weight'] = impute_weight(row['Physical-BMI'], row['Physical-Height'])\n","        \n","        return row\n","    \n","    return df.apply(impute_if_missing, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Apply to datasets\n","train_data = apply_imputation(train_data)\n","test_data = apply_imputation(test_data)\n","\n","# Check the results\n","print(\"Number of missing values after imputation:\")\n","print(\"BMI:\", train_data['Physical-BMI'].isna().sum())\n","print(\"Height:\", train_data['Physical-Height'].isna().sum())\n","print(\"Weight:\", train_data['Physical-Weight'].isna().sum())\n","\n","print(\"\\nSample of imputed BMI, Height, and Weight values:\")\n","imputed_sample = train_data[\n","    (train_data['Physical-BMI'].notnull() | train_data['Physical-Height'].notnull() | train_data['Physical-Weight'].notnull()) &\n","    ((train_data['Physical-BMI'].notnull() != train_data['Physical-BMI'].notnull().shift()) |\n","     (train_data['Physical-Height'].notnull() != train_data['Physical-Height'].notnull().shift()) |\n","     (train_data['Physical-Weight'].notnull() != train_data['Physical-Weight'].notnull().shift()))\n","].sample(5)[['Basic_Demos-Age', 'Basic_Demos-Sex', 'Physical-BMI', 'Physical-Height', 'Physical-Weight']]\n","print(imputed_sample)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additional analysis of imputation results\n","print(\"\\nImputation summary by age:\")\n","age_summary = train_data.groupby('Basic_Demos-Age').agg({\n","    'Physical-BMI': ['count', 'mean', 'std', 'min', 'max'],\n","    'Physical-Height': ['count', 'mean', 'std', 'min', 'max'],\n","    'Physical-Weight': ['count', 'mean', 'std', 'min', 'max'],\n","    'Basic_Demos-Sex': 'count'\n","})\n","print(age_summary)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Plot of imputed vs. original data\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-BMI', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('BMI vs Age (After Imputation)')\n","plt.savefig('analysis_output/bmi_vs_age_imputed.png')\n","plt.close()\n","\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-Height', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('Height vs Age (After Imputation)')\n","plt.savefig('analysis_output/height_vs_age_imputed.png')\n","plt.close()\n","\n","plt.figure(figsize=(12, 6))\n","sns.scatterplot(data=train_data, x='Basic_Demos-Age', y='Physical-Weight', hue='Basic_Demos-Sex', alpha=0.5)\n","plt.title('Weight vs Age (After Imputation)')\n","plt.savefig('analysis_output/weight_vs_age_imputed.png')\n","plt.close()\n","\n","print(\"Imputation analysis plots saved in the 'analysis_output' folder.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Export train_data to CSV\n","train_output_path = os.path.join(output_folder, 'train_data_imputed.csv')\n","train_data.to_csv(train_output_path, index=False)\n","print(f\"Imputed train data exported to: {train_output_path}\")\n","\n","# Export test_data to CSV\n","test_output_path = os.path.join(output_folder, 'test_data_imputed.csv')\n","test_data.to_csv(test_output_path, index=False)\n","print(f\"Imputed test data exported to: {test_output_path}\")\n","\n","print(\"Data export completed.\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Define categorical columns\n","category_columns =[\n","    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season',\n","    'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season',\n","    'PreInt_EduHx-Season'\n","]\n","\n","# Enumerate categorical columns\n","def create_mapping(column, dataset):\n","    unique_values = dataset[column].unique()\n","    return {value: idx for idx, value in enumerate(unique_values)}\n","\n","for col in category_columns:\n","    mapping = create_mapping(col, train_data)\n","    train_data[col] = train_data[col].replace(mapping).infer_objects(copy=False).astype(int)\n","    test_data[col] = test_data[col].replace(create_mapping(col, test_data)).infer_objects(copy=False).astype(int)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# High-risk vs. Low-risk analysis\n","# Suppress RuntimeWarnings\n","warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n","\n","# List of columns to drop\n","columns_to_drop = [\n","    'FGC-FGC_CU', 'FGC-FGC_PU', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone', \n","    'FGC-FGC_SRL', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', \n","    'Fitness_Endurance-Time_Sec', 'stat_41', 'stat_42', 'stat_39', 'stat_92_boxcox', 'stat_92',\n","    'stat_44', 'PCIAT-Season', 'PCIAT-PCIAT_Total'\n","]\n","\n","train_data = train_data.drop(columns=columns_to_drop, errors='ignore')\n","\n","# Separate high-risk and low-risk groups\n","high_risk = train_data[train_data['sii'] == 3]\n","low_risk = train_data[train_data['sii'] != 3]\n","\n","# Function to calculate summary statistics\n","def get_summary_stats(group):\n","    return pd.DataFrame({\n","        'mean': group.mean(),\n","        'median': group.median(),\n","        'std': group.std(),\n","        'min': group.min(),\n","        'max': group.max()\n","    })\n","\n","# List of columns to analyze (excluding 'sii' and non-numeric columns)\n","columns_to_analyze = train_data.select_dtypes(include=[np.number]).columns.drop('sii')\n","\n","# Calculate summary statistics for both groups\n","high_risk_stats = get_summary_stats(high_risk[columns_to_analyze])\n","low_risk_stats = get_summary_stats(low_risk[columns_to_analyze])\n","\n","# Calculate the difference in means\n","mean_diff = high_risk_stats['mean'] - low_risk_stats['mean']\n","\n","# Perform statistical tests to check for significant differences\n","def perform_statistical_test(col):\n","    try:\n","        # Try t-test first\n","        t_stat, p_val = stats.ttest_ind(high_risk[col].dropna(), low_risk[col].dropna(), equal_var=False)\n","        test_type = 't-test'\n","    except Exception:\n","        # If t-test fails, use Mann-Whitney U test\n","        try:\n","            u_stat, p_val = stats.mannwhitneyu(high_risk[col].dropna(), low_risk[col].dropna(), alternative='two-sided')\n","            test_type = 'Mann-Whitney U'\n","        except Exception:\n","            # If both tests fail, return NaN values\n","            return pd.Series({'statistic': np.nan, 'p_value': np.nan, 'test_type': 'Failed'})\n","    \n","    return pd.Series({'statistic': t_stat if test_type == 't-test' else u_stat, \n","                      'p_value': p_val, \n","                      'test_type': test_type})\n","\n","test_results = pd.DataFrame({col: perform_statistical_test(col) for col in columns_to_analyze}).T\n","\n","# Combine results\n","comparison_results = pd.concat([\n","    high_risk_stats.add_prefix('high_risk_'),\n","    low_risk_stats.add_prefix('low_risk_'),\n","    mean_diff.rename('mean_difference'),\n","    test_results\n","], axis=1)\n","\n","# Sort by absolute mean difference\n","comparison_results = comparison_results.sort_values('mean_difference', key=abs, ascending=False)\n","\n","# Display the top 20 most different features\n","print(comparison_results.head(20))"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Visualize distributions for top features\n","def plot_distribution(feature):\n","    plt.figure(figsize=(10, 6))\n","    sns.histplot(data=train_data, x=feature, hue='sii', kde=True, common_norm=False)\n","    plt.title(f'Distribution of {feature} by Risk Group')\n","    plt.savefig(f'analysis_output/distribution_{feature}.png')\n","    plt.close()\n","\n","for feature in comparison_results.head(20).index:\n","    plot_distribution(feature)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["train_data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Correlation analysis\n","def get_correlations(group):\n","    return group.corr().iloc[:, 0].sort_values(key=abs, ascending=False)\n","\n","high_risk_corr = get_correlations(high_risk[columns_to_analyze])\n","low_risk_corr = get_correlations(low_risk[columns_to_analyze])\n","\n","print(\"\\nTop correlations in high-risk group:\")\n","print(high_risk_corr.head(20))\n","print(\"\\nTop correlations in low-risk group:\")\n","print(low_risk_corr.head(20))\n","\n","# Calculate correlation differences\n","corr_diff = high_risk_corr - low_risk_corr\n","corr_diff = corr_diff.sort_values(key=abs, ascending=False)\n","\n","print(\"\\nTop correlation differences (high-risk minus low-risk):\")\n","print(corr_diff.head(20))\n","\n","# Save correlation results to CSV\n","pd.DataFrame({\n","    'high_risk_corr': high_risk_corr,\n","    'low_risk_corr': low_risk_corr,\n","    'correlation_difference': corr_diff\n","}).to_csv('analysis_output/correlation_comparison.csv')\n","\n","# Save results to CSV\n","comparison_results.to_csv('analysis_output/high_low_risk_comparison.csv')\n","\n","print(\"\\nAnalysis complete. Results saved to 'analysis_output/high_low_risk_comparison.csv' and 'analysis_output/correlation_comparison.csv'\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Visualize top correlation differences\n","plt.figure(figsize=(16, 12))\n","corr_diff.head(20).plot(kind='bar')\n","plt.title('Top 20 Correlation Differences (High-risk minus Low-risk)')\n","plt.xlabel('Features')\n","plt.ylabel('Correlation Difference')\n","plt.tight_layout()\n","plt.savefig('analysis_output/top_correlation_differences.png')\n","plt.close()\n","\n","print(\"Correlation difference plot saved to 'analysis_output/top_correlation_differences.png'\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Identify common columns and train-only columns\n","common_columns = list(set(train_data.columns) & set(test_data.columns))\n","train_only_columns = list(set(train_data.columns) - set(test_data.columns))\n","\n","# Remove 'sii' from feature columns if present\n","if 'sii' in common_columns:\n","    common_columns.remove('sii')\n","if 'sii' in train_only_columns:\n","    train_only_columns.remove('sii')\n","\n","print(f\"Number of common columns: {len(common_columns)}\")\n","print(f\"Number of train-only columns: {len(train_only_columns)}\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["# Separate features and target\n","X = train_data[common_columns]\n","y = train_data['sii']\n","X_test = test_data[common_columns]\n","\n","y = y.fillna(y.mode()[0])\n","\n","# KNN Imputation\n","imputer = KNNImputer(n_neighbors=5)\n","X_imputed = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n","X_test_imputed = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Calculate correlation differences\n","high_risk = X_imputed[y == 3]\n","low_risk = X_imputed[y != 3]\n","\n","high_risk_corr = high_risk.corr().abs()\n","low_risk_corr = low_risk.corr().abs()\n","\n","corr_diff = (high_risk_corr - low_risk_corr).abs()\n","mean_corr_diff = corr_diff.mean().sort_values(ascending=False)\n","top_corr_diff_features = mean_corr_diff.head(20).index.tolist() # Select the top 20 features\n","\n","# Main feature selection process\n","correlation_threshold = 0.3\n","corr_with_target = X_imputed.corrwith(y).abs()\n","corr_selected_features = corr_with_target[corr_with_target > correlation_threshold].index.tolist()\n","\n","# T test\n","def calculate_t_test(feature):\n","    high_risk_values = high_risk[feature]\n","    low_risk_values = low_risk[feature]\n","    _, p_value = stats.ttest_ind(high_risk_values, low_risk_values)\n","    return p_value\n","\n","significant_difference_threshold = 0.05\n","diff_selected_features = [feature for feature in X_imputed.columns if calculate_t_test(feature) < significant_difference_threshold]\n","\n","# Statistical feature selection\n","selector = SelectKBest(score_func=f_classif, k=50)\n","selector.fit(X_imputed, y)\n","stat_selected_features = X_imputed.columns[selector.get_support()].tolist()\n","\n","all_selected_features = list(set(corr_selected_features + \n","                                 diff_selected_features + \n","                                 top_corr_diff_features + \n","                                 stat_selected_features))\n","\n","domain_specific_features = ['PreInt_EduHx-computerinternet_hoursday', 'Basic_Demos-Age']\n","all_selected_features += domain_specific_features\n","all_selected_features = list(set(all_selected_features))\n","\n","print(f\"Total number of selected features: {len(all_selected_features)}\")\n","print(\"Selected features:\", all_selected_features)"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["# Prepare datasets\n","X_selected = X_imputed[all_selected_features]\n","X_test_selected = X_test_imputed[all_selected_features]\n"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":2}
